{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ab1e3f",
   "metadata": {},
   "source": [
    "# 数据加载与预处理\n",
    "使用Hugging Face的datasets库加载NarrativeQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d120ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class NarrativeQALoader:\n",
    "    def __init__(self, split: str = \"test\"):\n",
    "        \"\"\"\n",
    "        初始化加载器\n",
    "        Args:\n",
    "            split: 'train', 'validation', 或 'test'\n",
    "        \"\"\"\n",
    "        print(f\"正在加载 NarrativeQA 数据集 ({split} split)...\")\n",
    "        # 直接从Hugging Face Hub加载\n",
    "        from pyprojroot import here\n",
    "        dataset_path = here() / \"dataset\"\n",
    "        self.dataset = datasets.load_dataset(\"narrativeqa\", split=split, cache_dir=dataset_path)\n",
    "\n",
    "    def get_data(self) -> Tuple[List[str], List[List[str]], List[str]]:\n",
    "        \"\"\"\n",
    "        提取评估所需的关键字段\n",
    "        Returns:\n",
    "            questions: 问题列表\n",
    "            references: 参考答案列表（每个元素为包含两个答案的列表）\n",
    "            doc_ids: 文档ID列表，用于后续关联原文\n",
    "        \"\"\"\n",
    "        questions = []\n",
    "        references = []\n",
    "        doc_ids = []\n",
    "\n",
    "        for item in self.dataset:\n",
    "            # 提取问题文本\n",
    "            q_text = item['question']['text']\n",
    "\n",
    "            # 提取参考答案\n",
    "            # NarrativeQA的结构中，answers是一个包含两个对象的列表\n",
    "            # 每个对象有 'text' 字段\n",
    "            ans_texts = [ans['text'] for ans in item['answers']]\n",
    "\n",
    "            # 提取文档ID\n",
    "            d_id = item['document']['id']\n",
    "\n",
    "            questions.append(q_text)\n",
    "            references.append(ans_texts)\n",
    "            doc_ids.append(d_id)\n",
    "\n",
    "        return questions, references, doc_ids\n",
    "\n",
    "    def get_corpus(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        提取文档全文（如果需要进行检索实验）\n",
    "        Returns:\n",
    "            corpus_dict: {doc_id: full_text}\n",
    "        \"\"\"\n",
    "        corpus_dict = {}\n",
    "        # 注意：NarrativeQA的全文在 document.text 字段\n",
    "        # 部分样本可能只包含摘要，需检查 kind 字段\n",
    "        for item in self.dataset:\n",
    "            doc_id = item['document']['id']\n",
    "            # 这里简化处理，直接取text字段。实际可能需要清洗Gutenberg headers\n",
    "            text = item['document']['text']\n",
    "            corpus_dict[doc_id] = text\n",
    "        return corpus_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe47ee",
   "metadata": {},
   "source": [
    "加载数据集，如果本地没有数据集会下载\n",
    "\n",
    "数据被分为三组：\n",
    "\n",
    "| train | validation | test  |\n",
    "| ----- | ----- | ----- |\n",
    "| 32747 | 3461  | 10557 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NarrativeQALoader(split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a73675",
   "metadata": {},
   "source": [
    "测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs, refs, ids = loader.get_data()\n",
    "print(f\"加载完成，共 {len(qs)} 个测试样本。\")\n",
    "print(f\"示例问题: {qs}\")\n",
    "print(f\"示例参考答案: {refs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca910415",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one)\n",
    "print(one.keys())  # ['document', 'question', 'answers']\n",
    "print(one.get('document').keys())\n",
    "print(one.get('document').get('summary'))\n",
    "print(one.get('document').get('id'))  # 获取文档id\n",
    "print(one.get('document').get('text'))  # 获取文档全文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440cda2",
   "metadata": {},
   "source": [
    "# 评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "class NarrativeQAEvaluator:\n",
    "    def __init__(self):\n",
    "        # 初始化 ROUGE Scorer\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        # 初始化 METEOR (通过Hugging Face evaluate加载)\n",
    "        self.meteor_metric = evaluate.load('meteor')\n",
    "        # BLEU平滑函数，用于处理短文本\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "\n",
    "    def normalize_answer(self, s: str) -> str:\n",
    "        \"\"\"\n",
    "        标准化答案文本：\n",
    "        1. 小写\n",
    "        2. 去除标点\n",
    "        3. 规范化空白字符\n",
    "        这是复现官方分数的关键步骤。\n",
    "        \"\"\"\n",
    "\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "\n",
    "        return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "    def evaluate_one_sample(self, prediction: str, references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        对单个样本计算所有指标\n",
    "        \"\"\"\n",
    "        # 1. 文本标准化\n",
    "        pred_norm = self.normalize_answer(prediction)\n",
    "        refs_norm = [self.normalize_answer(r) for r in references]\n",
    "\n",
    "        # 2. Tokenization (用于BLEU)\n",
    "        pred_tokens = word_tokenize(pred_norm)\n",
    "        refs_tokens = [word_tokenize(r) for r in refs_norm]\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        # --- ROUGE-L (Max over references) ---\n",
    "        # 对每个参考答案分别计算ROUGE-L，取F-measure最大值\n",
    "        rouge_scores = [self.rouge_scorer.score(r, pred_norm)['rougeL'].fmeasure for r in refs_norm]\n",
    "        metrics = max(rouge_scores) if rouge_scores else 0.0\n",
    "\n",
    "        # --- BLEU-1 & BLEU-2 (Max over references) ---\n",
    "        # 尽管sentence_bleu支持多个references输入列表，但为了严格遵循Max策略，\n",
    "        # 且为了与部分文献对齐，这里显式对每对计算后取最大。\n",
    "        # 另一种常见做法是直接传入refs_tokens列表作为reference corpus，这会计算corpus level micro-average。\n",
    "        # 针对NarrativeQA，DeepMind官方脚本计算的是 micro-averaged 还是 max? \n",
    "        # 官方脚本通常使用 mteval-v13a.pl，它是计算 corpus level 的。\n",
    "        # 但在Python复现中，对每个样本取最大值也是一种稳健的 approximated sentence-level 方法。\n",
    "        # 下面演示 Sentence-Level Max 策略：\n",
    "\n",
    "        b1_scores = [sentence_bleu([ref], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=self.smoothing) for ref\n",
    "                     in refs_tokens]\n",
    "        metrics = max(b1_scores) if b1_scores else 0.0\n",
    "\n",
    "        b2_scores = [sentence_bleu([ref], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=self.smoothing) for\n",
    "                     ref in refs_tokens]\n",
    "        metrics = max(b2_scores) if b2_scores else 0.0\n",
    "\n",
    "        # --- METEOR ---\n",
    "        # METEOR 本身设计支持多参考答案，内部会自动处理对齐。\n",
    "        # 我们可以直接将所有references传给metric。\n",
    "        # 注意：evaluate库的compute方法通常是batch处理，这里为了逻辑清晰单条调用，\n",
    "        # 实际生产中应批量调用以提高速度。\n",
    "        # 由于evaluate接口限制，稍后在batch_evaluate中统一处理METEOR。\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def batch_evaluate(self, predictions: List[str], references: List[List[str]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        批量评估整个数据集\n",
    "        \"\"\"\n",
    "        total_scores = {'ROUGE-L': 0, 'BLEU-1': 0, 'BLEU-2': 0, 'METEOR': 0}\n",
    "\n",
    "        # 预处理数据用于METEOR批量计算\n",
    "        clean_preds = [self.normalize_answer(p) for p in predictions]\n",
    "        clean_refs = [[self.normalize_answer(r) for r in ref_list] for ref_list in references]\n",
    "\n",
    "        # 批量计算 METEOR (利用evaluate库的优化)\n",
    "        print(\"正在计算 METEOR...\")\n",
    "        meteor_res = self.meteor_metric.compute(predictions=clean_preds, references=clean_refs)\n",
    "        # 这一步返回的是corpus level的平均分，还是每个sentence的分数？\n",
    "        # evaluate的meteor返回 {'meteor': float}，是整个语料的平均分。\n",
    "        # 这与ROUGE/BLEU的sentence-level average略有不同，但在报告中作为系统级得分为标准做法。\n",
    "        total_scores = meteor_res['meteor']\n",
    "\n",
    "        # 逐个计算 ROUGE 和 BLEU\n",
    "        print(\"正在计算 ROUGE 和 BLEU...\")\n",
    "        for i, (p, r) in enumerate(zip(predictions, references)):\n",
    "            res = self.evaluate_one_sample(p, r)\n",
    "            total_scores.append(res)\n",
    "            total_scores.append(res)\n",
    "            total_scores.append(res)\n",
    "\n",
    "        # 汇总结果\n",
    "        final_metrics = {\n",
    "            'ROUGE-L': np.mean(total_scores),\n",
    "            'BLEU-1': np.mean(total_scores),\n",
    "            'BLEU-2': np.mean(total_scores),\n",
    "            'METEOR': total_scores\n",
    "        }\n",
    "\n",
    "        return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c25eb",
   "metadata": {},
   "source": [
    "# RAG系统与主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGSystem:\n",
    "    def __init__(self):\n",
    "        # 初始化检索器和LLM\n",
    "        # e.g., self.retriever = Chroma(...)\n",
    "        # e.g., self.llm = OpenAI(...)\n",
    "        pass\n",
    "\n",
    "    def generate(self, question: str, doc_id: str) -> str:\n",
    "        \"\"\"\n",
    "        模拟生成过程。\n",
    "        此处仅为了演示代码跑通，返回一个随机的占位符答案。\n",
    "        实际应为：\n",
    "        1. chunks = self.retriever.query(question, filter={doc_id: doc_id})\n",
    "        2. prompt = build_prompt(chunks, question)\n",
    "        3. answer = self.llm(prompt)\n",
    "        \"\"\"\n",
    "        return \"This is a generated answer based on the retrieved context.\"\n",
    "\n",
    "\n",
    "# 主执行脚本\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 准备数据\n",
    "    loader = NarrativeQALoader(split=\"test\")\n",
    "    questions, references, doc_ids = loader.get_data()\n",
    "\n",
    "    # 缩小数据集用于快速测试 (例如前100条)\n",
    "    SAMPLE_SIZE = 100\n",
    "    questions = questions\n",
    "    references = references\n",
    "    doc_ids = doc_ids\n",
    "\n",
    "    # 2. 运行RAG系统生成预测\n",
    "    rag_system = SimpleRAGSystem()\n",
    "    predictions =\n",
    "\n",
    "    print(f\"开始生成预测 (共 {len(questions)} 条)...\")\n",
    "    for q, d_id in zip(questions, doc_ids):\n",
    "        # 实际生成\n",
    "        # pred = rag_system.generate(q, d_id)\n",
    "\n",
    "        # 演示用：为了让分数不为0，我们随机选取一个参考答案并添加噪声\n",
    "        import random\n",
    "\n",
    "        mock_pred = references[questions.index(q)] + \" extra words\"\n",
    "        predictions.append(mock_pred)\n",
    "\n",
    "    # 3. 运行评估\n",
    "    evaluator = NarrativeQAEvaluator()\n",
    "    results = evaluator.batch_evaluate(predictions, references)\n",
    "\n",
    "    # 4. 输出最终报告\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"NarrativeQA RAG 系统测评结果报告\")\n",
    "    print(\"=\" * 40)\n",
    "    # 使用Markdown表格格式输出\n",
    "    print(f\"| Metric | Score |\")\n",
    "    print(f\"| :--- | :--- |\")\n",
    "    print(f\"| ROUGE-L | {results:.4f} |\")\n",
    "    print(f\"| BLEU-1 | {results:.4f} |\")\n",
    "    print(f\"| BLEU-2 | {results:.4f} |\")\n",
    "    print(f\"| METEOR | {results:.4f} |\")\n",
    "    print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
