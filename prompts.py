from pydantic import BaseModel, Field
from typing import Literal, List, Union, Dict 
import inspect
import re


def build_system_prompt(instruction: str="", example: str="", pydantic_schema: str="") -> str:
    delimiter = "\n\n---\n\n"
    schema = f"Your answer should be in JSON and strictly follow this schema, filling in the fields in the order they are given:\n```\n{pydantic_schema}\n```"
    if example:
        example = delimiter + example.strip()
    if schema:
        schema = delimiter + schema.strip()
    
    system_prompt = instruction.strip() + schema + example
    return system_prompt

class RephrasedQuestionsPrompt:
    instruction = """
You are a question rephrasing system.
Your task is to break down a comparative question into individual questions for each company mentioned.
Each output question must be self-contained, maintain the same intent and metric as the original question, be specific to the respective company, and use consistent phrasing.
"""

    class RephrasedQuestion(BaseModel):
        """Individual question for a company"""
        company_name: str = Field(description="Company name, exactly as provided in quotes in the original question")
        question: str = Field(description="Rephrased question specific to this company")

    class RephrasedQuestions(BaseModel):
        """List of rephrased questions"""
        questions: List['RephrasedQuestionsPrompt.RephrasedQuestion'] = Field(description="List of rephrased questions for each company")

    pydantic_schema = '''
class RephrasedQuestion(BaseModel):
    """Individual question for a company"""
    company_name: str = Field(description="Company name, exactly as provided in quotes in the original question")
    question: str = Field(description="Rephrased question specific to this company")

class RephrasedQuestions(BaseModel):
    """List of rephrased questions"""
    questions: List['RephrasedQuestionsPrompt.RephrasedQuestion'] = Field(description="List of rephrased questions for each company")
'''

    example = r"""
Example:
Input:
Original comparative question: 'Which company had higher revenue in 2022, "Apple" or "Microsoft"?'
Companies mentioned: "Apple", "Microsoft"

Output:
{
    "questions": [
        {
            "company_name": "Apple",
            "question": "What was Apple's revenue in 2022?"
        },
        {
            "company_name": "Microsoft", 
            "question": "What was Microsoft's revenue in 2022?"
        }
    ]
}
"""

    user_prompt = "Original comparative question: '{question}'\n\nCompanies mentioned: {companies}"

    system_prompt = build_system_prompt(instruction, example)

    system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


class AnswerWithRAGContextSharedPrompt:
    instruction = """
You are a RAG (Retrieval-Augmented Generation) answering system.
Your task is to answer the given question based only on information from the company's annual report, which is uploaded in the format of relevant pages extracted using RAG.

Before giving a final answer, carefully think out loud and step by step. Pay special attention to the wording of the question.
- Keep in mind that the content containing the answer may be worded differently than the question.
- The question was autogenerated from a template, so it may be meaningless or not applicable to the given company.
"""
    instruction_mmlongbench = """
You are a RAG (Retrieval-Augmented Generation) answering system.
Your task is to answer the given question based only on information from the pdf report, which is uploaded in the format of relevant pages extracted using RAG.

Before giving a final answer, carefully think out loud and step by step. Pay special attention to the wording of the question.
- Keep in mind that the content containing the answer may be worded differently than the question.
- The question was autogenerated from a template, so it may be meaningless or not applicable to the given report.
"""

    user_prompt = """
Here is the context:
\"\"\"
{context}
\"\"\"

---

Here is the question:
"{question}"
"""




class AnswerWithRAGContextListPrompt: 
    instruction = AnswerWithRAGContextSharedPrompt.instruction_mmlongbench 
    user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

    class AnswerSchema(BaseModel):
        step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked.")
        
        reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

        relevant_pages: List[int] = Field(description="""
List of page numbers containing information directly used to answer the question. Include only:
- Pages with direct answers or explicit statements
- Pages with key information that strongly supports the answer
Do not include pages with only tangentially related information or weak connections to the answer.
At least one page should be included in the list.
""")
        
        final_answer: Union[List[Union[str, int, float]], Literal["Not answerable"]] = Field(description="""  
A list of values extracted from the context. Each value should be:
- For strings: exactly as it appears in the context
- For numbers: converted to appropriate type (int or float)
- For nested lists: maintain the original structure
- Return 'Not answerable' if information is not available in the context
""")

    pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

    example = r"""
Example:
Question: 
"What are the quarterly revenue figures for 'Apple Inc.' in 2022?"

Answer: 
{
"step_by_step_analysis": "1. The question asks for quarterly revenue figures for 'Apple Inc.' in 2022, which implies we need to find four distinct values corresponding to each quarter.\n2. Examining the context, we find a table titled 'Quarterly Financial Results' on page 45 that lists revenue figures for each quarter of 2022.\n3. The table shows: Q1 - 123.9B,Q2−97.3B, Q3 - 82.96B,Q4−90.15B. All values are in billions of USD.\n4. We verify these are indeed revenue figures by checking the column header and accompanying notes which confirm these are 'Net Sales' (equivalent to revenue).\n5. The values are extracted exactly as presented, converted to float type for consistency.",
"reasoning_summary": "The 'Quarterly Financial Results' table on page 45 provides the exact quarterly revenue figures for 2022, which are extracted and converted to float values.",
"relevant_pages": [45],
"final_answer": [123.9, 97.3, 82.96, 90.15]
}
"""

    system_prompt = build_system_prompt(instruction, example)
    system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


class AnswerWithRAGContextNonePrompt: 
    instruction = AnswerWithRAGContextSharedPrompt.instruction_mmlongbench + """
Answer Extraction Rules:
1. When answer exists:
   - Return the exact string from source
   - Preserve original formatting (capitalization, hyphenation, etc.)
   
2. When answer doesn't exist:
   - Return 'Not answerable' for:
     * Missing required data
     * Unavailable references
     * Time-sensitive data not yet available
     * Contradictory sources

Special Cases:
- Chart/table data: 
  * Must match exact numeric references (e.g. '5%' ≠ '5 percent')
  * Copy annotations verbatim without interpretation
"""


    user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

    class AnswerSchema(BaseModel):
        step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question.")
        
        reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

        relevant_pages: List[int] = Field(description="""
List of page numbers containing information directly used to answer the question. Include only:
- Pages with direct answers or explicit statements
- Pages with key information that strongly supports the answer
Do not include pages with only tangentially related information or weak connections to the answer.
At least one page should be included in the list.
""")

        final_answer: Union[str, Literal["Not answerable"]] = Field(description="""
A string value is expected as the answer.
- Extract exactly as it appears in the context
- Do not modify or summarize the text
- Return 'Not answerable' if information is not available in the context
""")

    pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

    example = r"""
Example 1 (Answerable Question):
Question: 
"According to the report, how do 5% of the Latinos see economic upward mobility for their children?"

Answer: 
{
"step_by_step_analysis": "1. The question asks about the perception of 5% of Latinos regarding their children's economic upward mobility, requiring a direct quote from the report.\n2. On page 5 of the document, there is a chart titled 'Perceptions of Economic Mobility Among Latino Populations'.\n3. The chart includes a data point labeled '5% of respondents' with an adjacent text box that states 'Less well-off' in bold font.\n4. Cross-referencing with the chart legend confirms this represents their view on children's future economic status.\n5. The phrase 'Less well-off' is extracted exactly as it appears, without modification or interpretation, as it directly answers the question.",
"reasoning_summary": "Page 5's chart explicitly shows that 5% of Latino respondents view their children's economic mobility as 'Less well-off', which is extracted verbatim from the source.",
"relevant_pages": [5],
"final_answer": "Less well-off"
}

Example 2 (Unanswerable Question):
Question: 
"What's the percentage of people who are democrats and voted in the last election compared to the entire population in 2024?"

Answer: 
{
"step_by_step_analysis": "1. Question requires two precise data points: democrat voters and total population\n2. Searched for '2024 election' references - none found\n3. Checked all demographic sections - no voting breakdown by party\n4. Verified document metadata - report finalized Q3 2023 (pre-election)\n5. Attempted alternative queries - no matching tables/charts\n6. Conclusion: Data unavailable in this report",
"reasoning_summary": "Document contains no 2024 election data (pre-dates election) and lacks democrat-specific voting percentages, making question unanswerable.",
"relevant_pages": [],
"final_answer": "Not answerable"
}

"""

    system_prompt = build_system_prompt(instruction, example)
    system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


class AnswerWithRAGContextIntPrompt: 
    instruction = AnswerWithRAGContextSharedPrompt.instruction_mmlongbench
    user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

    class AnswerSchema(BaseModel):
        step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to number validation.")
        
        reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

        relevant_pages: List[int] = Field(description="""
List of page numbers containing information directly used to answer the question. Include only:
- Pages with direct answers or explicit statements
- Pages with key information that strongly supports the answer
Do not include pages with only tangentially related information or weak connections to the answer.
At least one page should be included in the list.
""")

        final_answer: Union[int, Literal["Not answerable"]] = Field(description="""
An integer value is expected as the answer.
- Pay attention to units (thousands, millions, etc.) and adjust accordingly
- Round to nearest integer if necessary
- Return 'Not answerable' if:
  - The value is not an integer
  - Information is not available
  - Currency mismatch occurs
""")

    pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

    example = r"""
Example:
Question: 
"How many employees did 'Tesla Inc.' have at the end of 2022?"

Answer: 
{
"step_by_step_analysis": "1. The question asks for the number of Tesla Inc. employees at the end of 2022, which should be a whole number.\n2. On page 56 of the annual report, we find the statement: 'As of December 31, 2022, we employed approximately 127,855 full-time employees worldwide.'\n3. The number 127,855 is explicitly stated as the employee count.\n4. We verify this is a global total by checking the context which mentions 'worldwide'.\n5. No unit conversion is needed as this is already a direct count.",
"reasoning_summary": "Page 56 explicitly states Tesla employed 127,855 full-time employees worldwide as of December 31, 2022.",
"relevant_pages": [56],
"final_answer": 127855
}

"""

    system_prompt = build_system_prompt(instruction, example)
    system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


class AnswerWithRAGContextFloatPrompt: 
    instruction = AnswerWithRAGContextSharedPrompt.instruction_mmlongbench
    user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

    class AnswerSchema(BaseModel):
        step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to decimal precision.")
        
        reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

        relevant_pages: List[int] = Field(description="""
List of page numbers containing information directly used to answer the question. Include only:
- Pages with direct answers or explicit statements
- Pages with key information that strongly supports the answer
Do not include pages with only tangentially related information or weak connections to the answer.
At least one page should be included in the list.
""")

        final_answer: Union[float, Literal["Not answerable"]] = Field(description="""
A floating-point number is expected as the answer.
- Maintain original decimal precision from the context
- Pay attention to units (thousands, millions, etc.) and adjust accordingly
- Return 'Not answerable' if:
  - The value is not a number
  - Information is not available
  - Currency mismatch occurs
""")

    pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

    example = r"""
Example:
Question: 
"What was the gross profit margin percentage for 'NVIDIA Corporation' in Q3 2022?"

Answer: 
{
"step_by_step_analysis": "1. The question asks for NVIDIA's gross profit margin percentage in Q3 2022, which should be a decimal number.\n2. On page 32 of the quarterly report, we find the statement: 'Gross margin for the quarter was 53.6%, down from 56.1% in the prior quarter.'\n3. The value 53.6% is explicitly stated as the gross margin for the quarter.\n4. We verify this is for Q3 2022 by checking the report header and date.\n5. The percentage is converted to its decimal equivalent (53.6).",
"reasoning_summary": "Page 32 states NVIDIA's Q3 2022 gross margin was 53.6%, which is converted to the decimal value 53.6.",
"relevant_pages": [32],
"final_answer": 53.6
}

"""

    system_prompt = build_system_prompt(instruction, example)
    system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


class AnswerSchemaFixPrompt:
    system_prompt = """
You are a JSON formatter.
Your task is to format raw LLM response into a valid JSON object.
Your answer should always start with '{' and end with '}'
Your answer should contain only json string, without any preambles, comments, or triple backticks.
"""

    user_prompt = """
Here is the system prompt that defines schema of the json object and provides an example of answer with valid schema:
\"\"\"
{system_prompt}
\"\"\"

---

Here is the LLM response that not following the schema and needs to be properly formatted:
\"\"\"
{response}
\"\"\"
"""




class RerankingPrompt:
    system_prompt_rerank_single_block = """
You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

You will receive a query and retrieved text block related to that query. Your task is to evaluate and score the block based on its relevance to the query provided.

Instructions:

1. Reasoning: 
   Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

2. Relevance Score (0 to 1, in increments of 0.1):
   0 = Completely Irrelevant: The block has no connection or relation to the query.
   0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
   0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
   0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
   0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
   0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
   0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
   0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
   0.8 = Very Relevant: Strongly relates to the query and provides significant information.
   0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
   1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

3. Additional Guidance:
   - Objectivity: Evaluate block based only on their content relative to the query.
   - Clarity: Be clear and concise in your justifications.
   - No assumptions: Do not infer information beyond what's explicitly stated in the block.
"""

    system_prompt_rerank_multiple_blocks = """
You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

You will receive a query and several retrieved text blocks related to that query. Your task is to evaluate and score each block based on its relevance to the query provided.

Instructions:

1. Reasoning: 
   Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

2. Relevance Score (0 to 1, in increments of 0.1):
   0 = Completely Irrelevant: The block has no connection or relation to the query.
   0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
   0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
   0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
   0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
   0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
   0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
   0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
   0.8 = Very Relevant: Strongly relates to the query and provides significant information.
   0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
   1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

3. Additional Guidance:
   - Objectivity: Evaluate blocks based only on their content relative to the query.
   - Clarity: Be clear and concise in your justifications.
   - No assumptions: Do not infer information beyond what's explicitly stated in the block.
"""

class RetrievalRankingSingleBlock(BaseModel):
    """Rank retrieved text block relevance to a query."""
    reasoning: str = Field(description="Analysis of the block, identifying key information and how it relates to the query")
    relevance_score: float = Field(description="Relevance score from 0 to 1, where 0 is Completely Irrelevant and 1 is Perfectly Relevant")

class RetrievalRankingMultipleBlocks(BaseModel):
    """Rank retrieved multiple text blocks relevance to a query."""
    block_rankings: List[RetrievalRankingSingleBlock] = Field(
        description="A list of text blocks and their associated relevance scores."
    )
